{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML APIs as preprocessors for scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import language\n",
    "from google.cloud.language import types\n",
    "from google.cloud.language import enums\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "client = bigquery.Client() \n",
    "%reload_ext google.cloud.bigquery\n",
    "\n",
    "pd.set_option(\"max_r\",6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load results from BigQuery into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery df\n",
    "SELECT source, title FROM \n",
    "`sgreenberg-project2.misc_ml.hacker_news_stories`\n",
    "ORDER BY id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First attempt to tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf=True)\n",
    "\n",
    "vocab = vectorizer.fit(df.title).vocabulary_\n",
    "vocab_df = pd.DataFrame(vocab.items())\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Natural Language API to tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = language.LanguageServiceClient()\n",
    "\n",
    "def tokenize(title):\n",
    "    document = types.Document(content=title, \n",
    "                              type=enums.Document.Type.PLAIN_TEXT)\n",
    "    tokens = client.analyze_syntax(document).tokens\n",
    "    tokenized_text = \" \".join([t.text.content for t in tokens])\n",
    "    return tokenized_text\n",
    "\n",
    "df['tokenized_title'] = df.title.apply(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second attempt to tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.fit(df.tokenized_title).vocabulary_\n",
    "vocab_df = pd.DataFrame(vocab.items())\n",
    "vocab_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
